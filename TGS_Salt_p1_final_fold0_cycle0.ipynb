{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b17c6c67116856c01c9bf00c1513f58681c8e8d9"
   },
   "source": [
    "    Phase 1 directly trained w/ LovaszHingeLoss. \n",
    "    Model: dsDSEUNeXt, no dropout\n",
    "    Fold: 0/4 (stratified)\n",
    "    lr_scheduler: CosineAnnealing\n",
    "    Optimizer: SGD\n",
    "    initial lr: 0.1\n",
    "    cycle: 0\n",
    "    batch_size: 16 \n",
    "    Epochs: 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "source": [
    "# TGS Salt Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "print(torch.__version__) # Different PyTorch version for CPU-only/GPU\n",
    "print(sys.version)\n",
    "\n",
    "torch.set_printoptions(precision = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f634a3996f5cfea4362c831336211936abf34de"
   },
   "source": [
    "## Data Visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76d7ffba733e2fac9977138bb82af4ec61daa36d"
   },
   "outputs": [],
   "source": [
    "!ls -la ../input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8dada8ef45203047fe332f2c2cdc90c3c105a0ef"
   },
   "outputs": [],
   "source": [
    "train_path = \"../input/train/\"\n",
    "\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "depths_df = pd.read_csv('../input/depths.csv')\n",
    "\n",
    "file_list = list(train_df['id'].values)\n",
    "\n",
    "train_df.columns, depths_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "607dbaf1c204c90f414bcbf14deb395650959c3d"
   },
   "outputs": [],
   "source": [
    "def plot_imgs_masks(file_list, train_path, depths_df, size = 1):\n",
    "    image_folder = os.path.join(train_path, \"images\")\n",
    "    mask_folder = os.path.join(train_path, \"masks\")\n",
    "    figs, axs = plt.subplots(2, size, figsize = (15, 8))\n",
    "    indices = np.random.randint(0, len(file_list), size = size)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        filename = file_list[idx]\n",
    "        image_path = os.path.join(image_folder, filename + \".png\")        \n",
    "        mask_path = os.path.join(mask_folder, filename + \".png\")\n",
    "        image = np.array(imageio.imread(image_path), dtype = np.uint8)\n",
    "        mask = np.array(imageio.imread(mask_path), dtype = bool).astype(np.float32)[..., np.newaxis]\n",
    "        depth = np.array(depths_df.loc[depths_df['id'] == filename]['z'], dtype = np.float32).reshape(1, 1, 1)\n",
    "        axs[0][i].imshow(image)\n",
    "        axs[0][i].set_title('depth: {}'.format(depth))\n",
    "        axs[0][i].grid()\n",
    "        axs[1][i].imshow(mask[:, :, 0])\n",
    "        axs[1][i].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db2d91387b8647bc3fcb772b8e1dccf520bf0fb1"
   },
   "outputs": [],
   "source": [
    "plot_imgs_masks(file_list, train_path, depths_df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "07535ed8d293a1f8cd0df667223862071b8ef0cc"
   },
   "outputs": [],
   "source": [
    "depths_df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "86ff372965cff2531a390f0a0adec1ad4bb5bf80"
   },
   "source": [
    "## Define Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e15c2dc69146a66dd7ae460f14769b9d4381a2ad"
   },
   "outputs": [],
   "source": [
    "class TGSSaltDataset(data.Dataset):    \n",
    "    def __init__(self, root_path, file_list, depths_df, train = True, augmentation = False):\n",
    "        \n",
    "        self.image_folder = os.path.join(root_path, \"images\")\n",
    "        self.mask_folder = os.path.join(root_path, \"masks\")\n",
    "        self.file_list = file_list\n",
    "        self.depths_df = depths_df\n",
    "        self.train = train\n",
    "        self.depth_range = self.depths_df['z'].max() - self.depths_df['z'].min()\n",
    "        self.depth_mean = self.depths_df['z'].mean()\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "        print('dataset size:', self.__len__())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def _transform(self, sample):\n",
    "\n",
    "        eps = 1e-5\n",
    "        \n",
    "        sample['image'] = TF.to_pil_image(sample['image'])\n",
    "        sample['image'] = TF.pad(sample['image'], padding = (13, 13, 14, 14), padding_mode = 'symmetric')\n",
    "        \n",
    "        if self.train:\n",
    "            sample['mask'] = TF.to_pil_image(sample['mask'])\n",
    "            sample['mask'] = TF.pad(sample['mask'], padding = (13, 13, 14, 14), padding_mode = 'symmetric')\n",
    "            \n",
    "            if self.augmentation:\n",
    "                if random.random() > 0.5:\n",
    "                    sample['image'] = TF.hflip(sample['image'])\n",
    "                    sample['mask'] = TF.hflip(sample['mask'])\n",
    "\n",
    "                if random.random() > 0.5:\n",
    "                    choice = np.random.choice(3)\n",
    "                    adjust = np.random.uniform(0.95, 1.05)\n",
    "                    if choice == 0:\n",
    "                        sample['image'] = TF.adjust_brightness(sample['image'], adjust)\n",
    "                    elif choice == 1:\n",
    "                        sample['image'] = TF.adjust_contrast(sample['image'], adjust)\n",
    "                    else:\n",
    "                        sample['image'] = TF.adjust_gamma(sample['image'], adjust)\n",
    "        \n",
    "            sample['image'] = TF.to_tensor(sample['image'])\n",
    "            sample['mask'] = TF.to_tensor(sample['mask'])\n",
    "            \n",
    "            '''\n",
    "                some images are all-zero. if not safe_division, then those empty images \n",
    "                would be damaged through normalization (due to zero std), hence prevent \n",
    "                network from improving.\n",
    "            '''\n",
    "            img_mean = sample['image'].mean()\n",
    "            img_std = sample['image'].std() + eps\n",
    "            sample['image'] = TF.normalize(sample['image'], [img_mean], [img_std])\n",
    "            \n",
    "            sample['empty_mask'] = torch.from_numpy(sample['empty_mask'])\n",
    "            \n",
    "        else:\n",
    "            # Test Time Augmentation\n",
    "            test_imgs = list()\n",
    "            test_imgs.append(TF.to_tensor(sample['image']))\n",
    "            if self.augmentation:\n",
    "                test_imgs.append(TF.to_tensor(TF.hflip(sample['image'])))\n",
    "                sample['depth'] = np.repeat(sample['depth'], 2, axis = 0)\n",
    "                \n",
    "            sample['image'] = torch.cat(test_imgs, dim = 0)\n",
    "\n",
    "            img_mean = sample['image'].view(len(test_imgs), -1).mean(dim = -1)\n",
    "            img_std = sample['image'].view(len(test_imgs), -1).std(dim = -1) + eps\n",
    "            sample['image'] = TF.normalize(sample['image'], img_mean, img_std).unsqueeze(1)  \n",
    "            sample['depth'] = np.expand_dims(sample['depth'], axis = 1)           \n",
    "        \n",
    "        sample['depth'] = torch.from_numpy((sample['depth'] - self.depth_mean) / (self.depth_range + eps))\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        file_idx = self.file_list[index]\n",
    "        \n",
    "        image_path = os.path.join(self.image_folder, file_idx + \".png\")\n",
    "        image = np.array(imageio.imread(image_path), dtype = np.uint8)[:, :, 0][..., np.newaxis]\n",
    "        depth = np.array(self.depths_df.loc[self.depths_df['id'] == file_idx]['z'], dtype = np.float32).reshape(1, 1, 1)\n",
    "        sample = {'image': image, 'depth': depth}\n",
    "\n",
    "        if self.train:\n",
    "            mask_path = os.path.join(self.mask_folder, file_idx + \".png\")\n",
    "            mask = np.array(imageio.imread(mask_path), dtype = bool).astype(np.float32)[..., np.newaxis]\n",
    "            sample['mask'] = mask\n",
    "            sample['empty_mask'] = (np.sum(mask) != 0).astype(np.float32).reshape(1)    # 0 for empty, 1 for non-empty\n",
    "\n",
    "        sample = self._transform(sample)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d01bc132fb32e18c8924202d6f9ffcf9f477b1d"
   },
   "outputs": [],
   "source": [
    "train_dataset = TGSSaltDataset(train_path, file_list, depths_df, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33e6fa62615f17eefeaf3ccddabbbed85a9c10f4"
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    sample = train_dataset[i]\n",
    "    print('data size: image:', sample['image'].shape, ' mask:', sample['mask'].shape)\n",
    "    print('image pixel value range:', (torch.min(sample['image']), torch.max(sample['image'])))\n",
    "    print('mask pixel value range:', (torch.min(sample['mask']), torch.max(sample['mask'])))\n",
    "    print('depth:', sample['depth'])\n",
    "    print('data type: image:', sample['image'].dtype, ' depth:', sample['depth'].dtype, ' mask:', sample['mask'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f1f5875d42a214fd85755fa903e438715498905"
   },
   "source": [
    "## Split Dataset & Define Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4900418fa1581b2896499f669365103383ab3232"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def get_dataloader(dataset, args, stratification_metrics = None):\n",
    "    \n",
    "    def random_split_indices(indices, split_ratio):\n",
    "        indices_perm = np.random.RandomState(seed = 42).permutation(indices)\n",
    "        train_size = int(len(indices_perm) * (1 - split_ratio))\n",
    "        valid_size = data_size - train_size\n",
    "        train_indices = indices_perm[: train_size]\n",
    "        valid_indices = indices_perm[-valid_size:]\n",
    "        return train_indices, valid_indices\n",
    "    \n",
    "    def k_fold_indices(indices, stratification_metrics, k):\n",
    "        skf = StratifiedKFold(n_splits = k, shuffle = True, random_state = 42)\n",
    "        for train_indices, valid_indices in skf.split(indices, stratification_metrics):\n",
    "#             print(train_indices.shape)\n",
    "            if len(train_indices) % args['batch_size'] == 1:\n",
    "                train_indices = np.pad(train_indices, (0, 1), 'edge')\n",
    "            if len(valid_indices) % args['batch_size'] == 1:\n",
    "                valid_indices = np.pad(valid_indices, (0, 1), 'edge')\n",
    "            yield train_indices, valid_indices\n",
    "    \n",
    "    if args['use_cuda']:\n",
    "        num_workers = 8\n",
    "        pin_memory = True\n",
    "    else:\n",
    "        num_workers = 1\n",
    "        pin_memory = False\n",
    "        \n",
    "    data_size = len(dataset)\n",
    "    \n",
    "    split_ratio = 0.2\n",
    "    \n",
    "    if args['k_fold'] is not None:\n",
    "        for indices in k_fold_indices(np.arange(data_size), stratification_metrics, args['k_fold']):\n",
    "            train_indices, valid_indices = indices\n",
    "            train_sampler = data.sampler.SubsetRandomSampler(train_indices)\n",
    "            valid_sampler = data.sampler.SubsetRandomSampler(valid_indices)\n",
    "            train_loader = data.DataLoader(dataset, batch_size = args['batch_size'], sampler = train_sampler, \n",
    "                                   pin_memory = args['use_cuda'], num_workers = num_workers)\n",
    "            valid_loader = data.DataLoader(dataset, batch_size = args['batch_size'], sampler = valid_sampler, \n",
    "                                   pin_memory = args['use_cuda'], num_workers = num_workers)\n",
    "            yield train_loader, valid_loader\n",
    "    else:\n",
    "        train_indices, valid_indices = random_split_indices(np.arange(data_size), split_ratio)\n",
    "        train_sampler = data.sampler.SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = data.sampler.SubsetRandomSampler(valid_indices)\n",
    "        train_loader = data.DataLoader(dataset, batch_size = args['batch_size'], sampler = train_sampler, \n",
    "                               pin_memory = args['use_cuda'], num_workers = num_workers)\n",
    "        valid_loader = data.DataLoader(dataset, batch_size = args['batch_size'], sampler = valid_sampler, \n",
    "                               pin_memory = args['use_cuda'], num_workers = num_workers)\n",
    "        yield train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79ca36df4bb7c03a3360964d1df8f8c11d4b8162"
   },
   "source": [
    "## Compute Coverage Class by mask & Create Stratified K-fold CV indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "107c50fd853039060416c841f157e391529d71c7"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def cov_to_class(val):\n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i:\n",
    "            return i\n",
    "\n",
    "coverage = list()\n",
    "for file_idx in tqdm(train_df['id']):\n",
    "    mask_path = os.path.join(train_dataset.mask_folder, file_idx + \".png\")\n",
    "    mask = np.array(imageio.imread(mask_path), dtype = bool).astype(np.float32)\n",
    "    coverage.append(np.mean(mask))\n",
    "train_df['coverage_class'] = list(map(cov_to_class, coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f5e71d12131f14f636c37be5f0f0b6579483522"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bfe92e8cbcd77a264d333f9ba5021e59b3004678"
   },
   "source": [
    "## Define Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2ab16f0a8952773573d4c2cab292ffaa913a292"
   },
   "outputs": [],
   "source": [
    "def training(model, criterions, optimizer, train_dataloaders, args, lr_scheduler = None, fold = None, \n",
    "             start_cycle = 0, model_dir = './model', verbose = True):\n",
    "    '''\n",
    "        params:\n",
    "            model: the model to be trained\n",
    "            criterion: loss function\n",
    "            optimizer: gradient descent optimizer\n",
    "            dataloaders: the generator of k-fold training set dataloaders (see function \n",
    "                         get_dataloaders)\n",
    "            args: arguments of training process\n",
    "            lr_scheduler: if provided, changes learning rate according to given rules\n",
    "            checkpoint: a string would be appended to the end of filename if provided\n",
    "            fold: if not None, only train the model on the specified fold split\n",
    "            model_dir: the directory path for saving/loading model\n",
    "    '''\n",
    "    if args['k_fold'] is None:\n",
    "        fold = None\n",
    "        \n",
    "    cycle = start_cycle    \n",
    "        \n",
    "    init_filename = model.__class__.__name__ + '_init'\n",
    "    \n",
    "    save_checkpoint({'model': model.state_dict(),\n",
    "                     'optimizer': optimizer.state_dict()\n",
    "                    }, False, model_dir, init_filename, verbose)\n",
    "    \n",
    "    print ('start training...')\n",
    "\n",
    "    if args['k_fold'] is not None and fold is None:\n",
    "        best_valid_metrics = {\n",
    "            'loss': [10e6] * args['k_fold'],\n",
    "            'accu': [0.0] * args['k_fold'],\n",
    "            'IoU': [0.0] * args['k_fold'],\n",
    "            'AvgPrec': [0.0] * args['k_fold']\n",
    "        }\n",
    "    else:\n",
    "        best_valid_metrics = {\n",
    "            'loss': [10e6],\n",
    "            'accu': [0.0],\n",
    "            'IoU': [0.0],\n",
    "            'AvgPrec': [0.0]\n",
    "        }\n",
    "\n",
    "    for fold_idx, dataloader in enumerate(train_dataloaders):\n",
    "\n",
    "        if fold is not None:\n",
    "            if fold_idx != fold:\n",
    "                continue\n",
    "\n",
    "        train_loader, valid_loader = dataloader\n",
    "\n",
    "        init_ckpt = load_checkpoint(False, model_dir, init_filename, verbose)\n",
    "        model.load_state_dict(init_ckpt['model'])\n",
    "        optimizer.load_state_dict(init_ckpt['optimizer'])\n",
    "\n",
    "        # for ReduceLROnPlateau only\n",
    "        if lr_scheduler is not None:\n",
    "            if lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
    "                lr_scheduler._reset()\n",
    "\n",
    "        metrics = {\n",
    "            'train_losses': [],\n",
    "            'train_accus': [],\n",
    "            'valid_losses': [],\n",
    "            'valid_accus': [],\n",
    "            'valid_ious': [],\n",
    "            'valid_avg_iou_precs': []\n",
    "        }\n",
    "\n",
    "        for epoch in np.arange(args['epochs']):\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                if lr_scheduler.__class__.__name__ != 'ReduceLROnPlateau':\n",
    "                    if lr_scheduler.__class__.__name__ == 'CosineAnnealingLR':\n",
    "                        cur_cycle = start_cycle + epoch // lr_scheduler.T_max\n",
    "                        if cycle != cur_cycle:\n",
    "                            cycle = cur_cycle\n",
    "                            best_valid_metrics = {\n",
    "                                'loss': [10e6],\n",
    "                                'accu': [0.0],\n",
    "                                'IoU': [0.0],\n",
    "                                'AvgPrec': [0.0]\n",
    "                            }\n",
    "                        if epoch % lr_scheduler.T_max == 0 and verbose:\n",
    "                            print('cycle {}:'.format(cycle))\n",
    "                        lr_scheduler.step(epoch % lr_scheduler.T_max)\n",
    "                    else:\n",
    "                        lr_scheduler.step(epoch)\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            train_loss = 0.0\n",
    "            train_accu = 0.0\n",
    "\n",
    "            for i, batch_data in enumerate(train_loader, 0):\n",
    "                samples = batch_data  \n",
    "                imgs = samples['image']\n",
    "                masks = samples['mask']\n",
    "                deps = samples['depth'] \n",
    "                labels = samples['empty_mask']\n",
    "                if args['use_cuda']:\n",
    "                    imgs, deps, masks, labels = imgs.cuda(), deps.cuda(), masks.cuda(), labels.cuda()\n",
    "#                 has_nan([imgs, deps, masks], 'training input for model: epoch: {}'.format(epoch + 1))\n",
    "                optimizer.zero_grad()\n",
    "                segs, binary = model((imgs, deps))\n",
    "#                 has_nan(segs, 'computing training output: epoch: {}'.format(epoch + 1))\n",
    "                ne_indices = (labels.squeeze(1) == 1)\n",
    "                ne_preds, ne_masks = segs[ne_indices], masks[ne_indices]\n",
    "                loss = criterions['all_segs'](segs, masks)\n",
    "                loss += 0.1 * criterions['binary'](binary.reshape(-1, 1), labels)\n",
    "                if len(ne_preds) > 0:\n",
    "                    loss += 0.1 * criterions['non_empty'](ne_preds, ne_masks)\n",
    "#                 has_nan(loss, 'computing training loss: epoch: {}'.format(epoch + 1))\n",
    "                train_loss += loss.detach()\n",
    "                if criterions['all_segs'].__class__.__name__ == 'LovaszHingeLoss':\n",
    "                    train_accu += torch.mean((torch.eq(torch.ge(segs.detach(), 0).float(), masks)).float())\n",
    "                else:\n",
    "                    train_accu += torch.mean((torch.eq(segs.detach().sigmoid().round(), masks)).float())\n",
    "\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            metrics['train_losses'].append(train_loss / (i + 1))\n",
    "            metrics['train_accus'].append(train_accu / (i + 1))\n",
    "\n",
    "            if verbose:\n",
    "                print('epoch [{0} / {1}]:'.format(epoch + 1, args['epochs']))\n",
    "                print('train:  \\nLoss: {:.4f}  Accu: {:.4f}'.format(train_loss / (i + 1), train_accu / (i + 1)))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            valid_accu = 0.0\n",
    "            valid_iou = 0.0\n",
    "            valid_avg_iou_prec = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, batch_data in enumerate(valid_loader, 0):\n",
    "                    samples = batch_data\n",
    "                    imgs = samples['image']\n",
    "                    masks = samples['mask']\n",
    "                    deps = samples['depth'] \n",
    "                    labels = samples['empty_mask']\n",
    "                    if args['use_cuda']:\n",
    "                        imgs, deps, masks, labels = imgs.cuda(), deps.cuda(), masks.cuda(), labels.cuda()\n",
    "#                     has_nan([imgs, deps, masks], 'validation input for model: epoch: {}'.format(epoch + 1))\n",
    "                    segs, binary = model((imgs, deps))\n",
    "#                     has_nan(segs, 'computing validation output: epoch: {}'.format(epoch + 1))\n",
    "                    ne_indices = (labels.squeeze(1) == 1)\n",
    "                    ne_preds, ne_masks = segs[ne_indices], masks[ne_indices]\n",
    "                    loss = criterions['all_segs'](segs, masks)\n",
    "                    loss += 0.1 * criterions['binary'](binary.reshape(-1, 1), labels)\n",
    "                    if len(ne_preds) > 0:\n",
    "                        loss += 0.1 * criterions['non_empty'](ne_preds, ne_masks)\n",
    "#                     has_nan(loss, 'computing validation loss: epoch: {}'.format(epoch + 1))\n",
    "                    valid_loss += loss.detach()\n",
    "                    if criterions['all_segs'].__class__.__name__ == 'LovaszHingeLoss':\n",
    "                        valid_accu += torch.mean((torch.eq(torch.ge(segs.detach(), 0).float(), masks)).float())\n",
    "                    else:\n",
    "                        valid_accu += torch.mean((torch.eq(segs.detach().sigmoid().round(), masks)).float())\n",
    "                    iou = avg_iou_precision(segs.detach(), masks.detach(), criterions['all_segs'].__class__.__name__)\n",
    "                    valid_iou += iou[0]\n",
    "                    valid_avg_iou_prec += iou[1]\n",
    "\n",
    "            metrics['valid_losses'].append(valid_loss / (i + 1))\n",
    "            metrics['valid_accus'].append(valid_accu / (i + 1))\n",
    "            metrics['valid_ious'].append(valid_iou / (i + 1))\n",
    "            metrics['valid_avg_iou_precs'].append(valid_avg_iou_prec / (i + 1))\n",
    "\n",
    "            if verbose:\n",
    "                print('valid:  \\nLoss: {:.4f}  Accu: {:.4f}  \\nIoU: {}  \\nAvgPrecision: {}'.\\\n",
    "                      format(valid_loss / (i + 1), valid_accu / (i + 1), valid_iou / (i + 1), \n",
    "                             valid_avg_iou_prec / (i + 1)))\n",
    "                print('epoch time: {}s'.format(int(time.time() - t0)))\n",
    "\n",
    "            '''\n",
    "                 the 'epoch' here is only for verbose print, hence we use 'epoch + 1' \n",
    "                 to indicate at the end of which epoch it reduces lr. In StepLR, we \n",
    "                 use epoch instead.\n",
    "            '''\n",
    "            if lr_scheduler is not None:\n",
    "                if lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
    "                    lr_scheduler.step(valid_iou / (i + 1), epoch = (epoch + 1))\n",
    "\n",
    "            if fold is not None:\n",
    "                idx = 0\n",
    "            else:\n",
    "                idx = fold_idx\n",
    "            if best_valid_metrics['IoU'][idx] < metrics['valid_ious'][-1]:\n",
    "                is_best = True\n",
    "                best_valid_metrics['loss'][idx] = metrics['valid_losses'][-1]\n",
    "                best_valid_metrics['accu'][idx] = metrics['valid_accus'][-1]\n",
    "                best_valid_metrics['IoU'][idx] = metrics['valid_ious'][-1]\n",
    "                best_valid_metrics['AvgPrec'][idx] = metrics['valid_avg_iou_precs'][-1]\n",
    "\n",
    "            if is_best or (epoch % 10 == 9) or (epoch == args['epochs'] - 1):\n",
    "                filename = model.__class__.__name__ + '_fold{}'.format(fold_idx) + '_cycle{}'.format(cycle)\n",
    "                save_checkpoint({'epoch': epoch + 1,\n",
    "                                 'model': model.state_dict(),\n",
    "                                 'metrics': metrics,\n",
    "                                 'best_valid_metrics': best_valid_metrics,\n",
    "                                 'optimizer': optimizer.state_dict()\n",
    "                                }, is_best, model_dir, filename, verbose)\n",
    "                is_best = False\n",
    "\n",
    "    summarize_CV(best_valid_metrics)\n",
    "\n",
    "    print('training finished.')\n",
    "    \n",
    "    del_checkpoint(False, model_dir, init_filename, verbose)\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "    \n",
    "\n",
    "# --------------------------------------- helper function ---------------------------------------\n",
    "\n",
    "\n",
    "def has_nan(t, id_str = 'test'):\n",
    "    '''\n",
    "        params:\n",
    "            t: torch.tensor or sequence of torch.tensor\n",
    "            id_str: identification string to print\n",
    "    '''\n",
    "    def _check(t, id_str):\n",
    "        if torch.sum(torch.isnan(t)):\n",
    "            print('NaNs tensor \\n{} \\nfound at {}'.format(t, id_str))\n",
    "            sys.exit()\n",
    "            \n",
    "    if torch.is_tensor(t):\n",
    "        _check(t, id_str)\n",
    "    else:\n",
    "        for i, item in enumerate(t):\n",
    "            _check(item, id_str + ' [{}/{}]'.format(i, len(t) - 1))\n",
    "        \n",
    "\n",
    "def summarize_CV(CV_dict):\n",
    "    metrics = list(CV_dict.keys())\n",
    "    print('Cross Validation on {}-fold: '.format(len(CV_dict[metrics[0]])))\n",
    "    for metric in metrics:\n",
    "        print('best {}:  mean: {:.4f} std: {:.4f}'.format(metric, np.mean(CV_dict[metric]), \n",
    "                                                          np.std(CV_dict[metric])))\n",
    "    \n",
    "    \n",
    "def avg_iou_precision(preds, targets, loss = None):\n",
    "    smooth = 10e-8\n",
    "    thresholds_iou = np.linspace(0.50, 0.95, 10)\n",
    "    if loss == 'LovaszHingeLoss':\n",
    "        threshold_pred = 0.0\n",
    "    else:\n",
    "        threshold_pred = 0.5\n",
    "    batch_size = preds.shape[0]\n",
    "    targets = targets.byte()\n",
    "    \n",
    "    avg_precision = torch.zeros((batch_size, 1), device = preds.device)\n",
    "    preds_t = torch.ge(preds, threshold_pred)\n",
    "    intersection = (preds_t & targets).float().sum(dim = (2, 3))\n",
    "    union = (preds_t | targets).float().sum(dim = (2, 3))\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    for t_i in thresholds_iou:\n",
    "        avg_precision += torch.ge(iou, t_i).float()\n",
    "    avg_precision = avg_precision / len(thresholds_iou)\n",
    "    return iou.mean(), avg_precision.mean()\n",
    "\n",
    "    \n",
    "def save_checkpoint(state: dict, is_best: bool, dir_path = './model', filename = 'ckpt', verbose = True):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    file_path = os.path.join(dir_path, filename + '.pth.tar')\n",
    "    torch.save(state, file_path)\n",
    "    if is_best:\n",
    "        best_file_path = os.path.join(dir_path, 'best_' + filename + '.pth.tar')\n",
    "        shutil.copyfile(file_path, best_file_path)\n",
    "        if verbose:\n",
    "            print('model saved at {}'.format(best_file_path))\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('model saved at {}'.format(file_path))\n",
    "        \n",
    "\n",
    "def load_checkpoint(is_best: bool, dir_path = './model', filename = 'ckpt', verbose = True):\n",
    "    if is_best:\n",
    "        file_path = os.path.join(dir_path, 'best_' + filename + '.pth.tar')\n",
    "    else:\n",
    "        file_path = os.path.join(dir_path, filename + '.pth.tar')\n",
    "    if os.path.isfile(file_path):\n",
    "        checkpoint = torch.load(file_path)\n",
    "        mtime = ''\n",
    "        if verbose:\n",
    "            mtime = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print (\"checkpoint loaded '{}' {}\".format(file_path, mtime))\n",
    "        return checkpoint\n",
    "    else:\n",
    "        print('checkpoint {} not found.'.format(file_path))\n",
    "        \n",
    "def del_checkpoint(is_best: bool, dir_path = './model', filename = 'ckpt', verbose = True):\n",
    "    if is_best:\n",
    "        file_path = os.path.join(dir_path, 'best_' + filename + '.pth.tar')\n",
    "    else:\n",
    "        file_path = os.path.join(dir_path, filename + '.pth.tar')\n",
    "    if os.path.isfile(file_path):\n",
    "        mtime = ''\n",
    "        if verbose:\n",
    "            mtime = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            print (\"checkpoint deleted '{}' {}\".format(file_path, mtime))   \n",
    "        os.remove(file_path)\n",
    "    else:\n",
    "        print('checkpoint {} not found.'.format(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "516542df09b3a37d69f0f527b98fd64c2708b48a"
   },
   "source": [
    "## Define Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a55184055742215e5895009ac1363b75f1c47199"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    vanilla conv2d with activation and batchnorm\n",
    "'''\n",
    "def _conv2d(in_channel, out_channel, kernel_size = 3, stride = 1, padding = 0, dilation = 1, \n",
    "            groups = 1, bn_acti = True):        \n",
    "    block = nn.Sequential(\n",
    "        nn.Conv2d(in_channel, out_channel, kernel_size, stride, padding, dilation, groups)\n",
    "    )\n",
    "    \n",
    "    if bn_acti:\n",
    "        block.add_module('batchnorm', nn.BatchNorm2d(out_channel))\n",
    "        block.add_module('activation', nn.ELU())\n",
    "    return block\n",
    "\n",
    "\n",
    "'''\n",
    "    transposed conv2d with activation and batchnorm\n",
    "'''\n",
    "def _tconv2d(in_channel, out_channel, kernel_size = 3, stride = 1, padding = 0, out_padding = 0, \n",
    "             dilation = 1, groups = 1, bn_acti = True):\n",
    "    block = nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channel, out_channel, kernel_size, stride, padding, out_padding, \n",
    "                           groups, dilation = dilation)\n",
    "    )\n",
    "    \n",
    "    if bn_acti:\n",
    "        block.add_module('batchnorm', nn.BatchNorm2d(out_channel))\n",
    "        block.add_module('activation', nn.ELU())\n",
    "    return block   \n",
    "\n",
    "\n",
    "'''\n",
    "    vanilla contracting block in UNet\n",
    "'''\n",
    "class _contract(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size = 3, stride = 1, padding = 0, dropout = None):\n",
    "        super(_contract, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        conv = list()\n",
    "        if dropout is not None:\n",
    "            conv = [nn.Dropout2d(p = dropout)]\n",
    "        conv += [_conv2d(in_channel, out_channel, kernel_size, stride, padding),\n",
    "                 _conv2d(out_channel, out_channel, kernel_size, stride, padding)]\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.conv(self.maxpool(x))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "'''\n",
    "    contracting block with: Maxpool -> Inception module as feature map size reduction\n",
    "                            conv2d -> ResBlock as feature map increasing\n",
    "'''\n",
    "class _contract_v2(nn.Module):\n",
    "    def __init__(self, in_channel, incept_channels, dim_reduces, inter_channel, out_channel, kernel_size = 3, \n",
    "                 stride = 1, padding = 0, dilation = 1, groups = 1, num_res = 2, dropout = None, scSE = False):\n",
    "        super(_contract_v2, self).__init__()\n",
    "        '''\n",
    "            This module has two output heads. One is placed right after the Inception module \n",
    "            and the other is placed at the end.\n",
    "            \n",
    "            params:\n",
    "                in_channel: input channel for Inception module\n",
    "                out_channels: a dict of out_channel for Inception module\n",
    "                dim_reduces: a dict of intermidiate feature map dimension reduction with conv1 \n",
    "                             before conv3 and conv5\n",
    "                kernel_size: kernel_size for last 3 ResBlocks (kernel_size for Inception module \n",
    "                             is fixed)\n",
    "                stride: stride for last 3 ResBlocks (stride for Inception module is fixed)\n",
    "                padding: padding for last two ResBlocks (padding for Inception module is fixed)\n",
    "                num_res: number of resblocks after Inception module\n",
    "                dropout: a intermidiate 2d dropout layer placed before ResBlocks\n",
    "                scSE: whether to add scSE blocks at the end\n",
    "                \n",
    "            We intend to maintain in_channel = sum(out_channels.values()) to have the Inception \n",
    "            module act like a Maxpooling layer, however, it seems a little bit hard to do this.\n",
    "            Hence, the sum(out_channels.values()) can be slightly larger than in_channel.\n",
    "            The actual output channel for the last ResBlock = 2 * sum(out_channels.values())\n",
    "        '''\n",
    "        if not isinstance(padding, list):\n",
    "            padding = [padding] * num_res\n",
    "        if not isinstance(dilation, list):\n",
    "            dilation = [dilation] * num_res\n",
    "        if not isinstance(groups, list):\n",
    "            groups = [groups] * num_res\n",
    "        if not isinstance(inter_channel, list):\n",
    "            inter_channel = [inter_channel] * num_res\n",
    "        channels = [(sum(incept_channels.values()), inter_channel[0], out_channel)] + \\\n",
    "                   [(out_channel, inter_channel[i], out_channel) for i in range(num_res - 1)]\n",
    "        \n",
    "        self.Inception = nn.Sequential(\n",
    "            _InceptionModule(in_channel, incept_channels, dim_reduces, stride = 2)\n",
    "        )\n",
    "        ResBlocks = list()\n",
    "        if dropout is not None:\n",
    "            ResBlocks = [nn.Dropout2d(dropout)]\n",
    "        ResBlocks += [_ResBlock(*channels[i], kernel_size, stride, padding[i], dilation[i], groups[i]) \n",
    "                      for i in range(num_res)]\n",
    "        self.ResBlocks = nn.Sequential(*ResBlocks)\n",
    "        \n",
    "        if scSE:\n",
    "            self.Inception.add_module('scSE', _scSE(sum(incept_channels.values())))\n",
    "            self.ResBlocks.add_module('scSE', _scSE(out_channel))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inception = self.Inception(x)  \n",
    "        output = self.ResBlocks(inception)\n",
    "        \n",
    "        return inception, output\n",
    "\n",
    "    \n",
    "'''\n",
    "    vanilla expanding block in UNet\n",
    "'''\n",
    "class _expand(nn.Module):\n",
    "    def __init__(self, in_channel, cat_channel, out_channel, kernel_size = 3, stride = 1, padding = 0, dropout = None):\n",
    "        super(_expand, self).__init__()\n",
    "        self.tconv = _tconv2d(in_channel, int(in_channel / 2), kernel_size = 2, stride = 2, bn_acti = False)\n",
    "        conv = list()\n",
    "        if dropout is not None:\n",
    "            conv = [nn.Dropout2d(p = dropout)]\n",
    "        conv += [_conv2d(int(in_channel / 2) + cat_channel, out_channel, kernel_size, stride, padding),\n",
    "                 _conv2d(out_channel, out_channel, kernel_size, stride, padding)]\n",
    "        self.conv = nn.Sequential(*conv)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        output = self.conv(torch.cat([self.tconv(x), skip], dim = 1))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "'''\n",
    "    expanding block with: tconv2d -> tconv2d as feature map upsampling\n",
    "                          conv2d -> ResBlock as feature map increasing\n",
    "'''    \n",
    "class _expand_v2(nn.Module):\n",
    "    def __init__(self, in_channel, cat_channel, inter_channel, out_channel, kernel_size = 3, stride = 1, \n",
    "                 padding = 0, dilation = 1, groups = 1, num_res = 2, dropout = None, scSE = False):\n",
    "        super(_expand_v2, self).__init__()\n",
    "        '''\n",
    "            params:\n",
    "                in_channel: input channel for tconv2d layer\n",
    "                cat_channel: channel for feature maps to be concatenated with the input\n",
    "                out_channel: out_channel for last ResBlock\n",
    "                kernel_size: kernel_size for last 3 ResBlocks (kernel_size for tconv2d layer \n",
    "                             is fixed)\n",
    "                stride: stride for last 3 ResBlocks (stride for tconv2d layer is fixed)\n",
    "                padding: padding for last 3 ResBlocks (padding for tconv2d layer is fixed)\n",
    "                num_res: number of resblocks after tconv2d layer\n",
    "                dropout: a intermidiate 2d dropout layer placed before ResBlocks\n",
    "                scSE: whether to add scSE blocks at the end\n",
    "        '''\n",
    "        if not isinstance(padding, list):\n",
    "            padding = [padding] * num_res\n",
    "        if not isinstance(dilation, list):\n",
    "            dilation = [dilation] * num_res\n",
    "        if not isinstance(groups, list):\n",
    "            groups = [groups] * num_res\n",
    "        if not isinstance(inter_channel, list):\n",
    "            inter_channel = [inter_channel] * num_res\n",
    "        channels = [(int(in_channel / 2) + cat_channel, inter_channel[0], out_channel)] + \\\n",
    "                   [(out_channel, inter_channel[i], out_channel) for i in range(num_res - 1)]\n",
    "        \n",
    "        \n",
    "        self.tconv = nn.Sequential(\n",
    "            _tconv2d(in_channel, int(in_channel / 2), kernel_size = 2, stride = 2, bn_acti = False)\n",
    "        )\n",
    "        ResBlocks = list()\n",
    "        if dropout is not None:\n",
    "            ResBlocks = [nn.Dropout2d(dropout)]\n",
    "        ResBlocks += [_ResBlock(*channels[i], kernel_size, stride, padding[i], dilation[i], groups[i]) \n",
    "                      for i in range(num_res)]\n",
    "        self.ResBlocks = nn.Sequential(*ResBlocks)\n",
    "        \n",
    "        if scSE:\n",
    "            self.ResBlocks.add_module('scSE', _scSE(out_channel))\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        tconv = self.tconv(x)\n",
    "        output = self.ResBlocks(torch.cat([tconv, skip], dim = 1))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "\n",
    "'''\n",
    "    SE blocks\n",
    "'''\n",
    "class _cSE(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(_cSE, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_channel, int(in_channel / 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(int(in_channel / 2), in_channel),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_identity = x\n",
    "        x = x.reshape(*(x.shape[: 2]), -1).mean(dim = -1)\n",
    "        output = x_identity * self.project(x).reshape(-1, self.in_channel, 1, 1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class _sSE(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(_sSE, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.project = nn.Sequential(\n",
    "            _conv2d(in_channel, 1, kernel_size = 1, bn_acti = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_identity = x\n",
    "        output = x_identity * self.project(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class _scSE(nn.Module):\n",
    "    def __init__(self, in_channel):\n",
    "        super(_scSE, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.cSE = _cSE(self.in_channel)\n",
    "        self.sSE = _sSE(self.in_channel)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.cSE(x) + self.sSE(x)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "'''\n",
    "    zero padding input for skip connection in residual block\n",
    "'''\n",
    "def _zero_padding(x, target_dim):\n",
    "    padding_dim = target_dim - x.shape[1]\n",
    "    if padding_dim > 0:\n",
    "        padding_shape = (x.shape[0], padding_dim, x.shape[2], x.shape[3])\n",
    "        zero_padding = torch.zeros(padding_shape, dtype = x.dtype, device = x.device)\n",
    "        x_padded = torch.cat([x, zero_padding], dim = 1)\n",
    "        return x_padded\n",
    "    return x\n",
    "\n",
    "\n",
    "'''\n",
    "    parallel conv2d with different kernel size concatenated together\n",
    "'''\n",
    "class _MultiConv2d(nn.Module):\n",
    "    def __init__(self, in_channel, out_channels, kernel_sizes, paddings, strides = None, bn_acti = True):\n",
    "        super(_MultiConv2d, self).__init__()\n",
    "        assert len(kernel_sizes) == len(paddings), 'inconsistent number of args specified'\n",
    "        \n",
    "        self.in_channel = in_channel\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.paddings = paddings\n",
    "        self.strides = strides\n",
    "        self.bn_acti = bn_acti\n",
    "        \n",
    "        if strides is None:\n",
    "            self.strides = [1] * len(kernel_sizes)\n",
    "        \n",
    "        if isinstance(out_channels, int):\n",
    "            self.out_channels = [out_channels] * len(kernel_sizes)\n",
    "        \n",
    "        self.conv2d_layers = nn.ModuleList([_conv2d(in_channel, out_channels[i], kernel_sizes[i], strides[i], \n",
    "                                                    paddings[i], bn_acti = False) for i in range(len(kernel_sizes))])\n",
    "        \n",
    "        self.bn_acti_layers = nn.Sequential()\n",
    "        if self.bn_acti:\n",
    "            self.bn_acti_layers.add_module('batchnorm', nn.BatchNorm2d(sum(self.out_channels)))\n",
    "            self.bn_acti_layers.add_module('activation', nn.ELU())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = list()\n",
    "        for layer in self.conv2d_layers:\n",
    "            output.append(layer(x))\n",
    "        \n",
    "        output = self.bn_acti_layers(torch.cat(output, dim = 1))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "'''\n",
    "    residual (ResNet/ResNeXt) block in ResNeXt\n",
    "'''\n",
    "class _ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, inter_channel, out_channel, kernel_size, stride = 1, padding = 0, \n",
    "                 dilation = 1, groups = 1, dropout = None, bn_acti = True):\n",
    "        super(_ResBlock, self).__init__()\n",
    "        self.in_channel = in_channel\n",
    "        self.inter_channel = inter_channel\n",
    "        self.out_channel = out_channel\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.groups = groups\n",
    "        self.dilation = dilation\n",
    "        self.dropout_rate = dropout\n",
    "        self.bn_acti = bn_acti\n",
    "        \n",
    "        self.paths = nn.Sequential(\n",
    "            _conv2d(in_channel, inter_channel, kernel_size = 1),\n",
    "            _conv2d(inter_channel, inter_channel, kernel_size = kernel_size, stride = stride, \n",
    "                    padding = padding, dilation = dilation, groups = groups),\n",
    "            _conv2d(inter_channel, out_channel, kernel_size = 1, bn_acti = False)\n",
    "        )\n",
    "        \n",
    "        self.identity = nn.Sequential()\n",
    "        if in_channel != out_channel:\n",
    "            self.identity = _conv2d(in_channel, out_channel, kernel_size = 1, stride = stride, bn_acti = False)\n",
    "        \n",
    "        self.bn_acti_layers = nn.Sequential()\n",
    "        if self.bn_acti:\n",
    "            self.bn_acti_layers.add_module('batchnorm', nn.BatchNorm2d(out_channel))\n",
    "            self.bn_acti_layers.add_module('activation', nn.ELU())\n",
    "            \n",
    "        self.dropout = nn.Sequential()\n",
    "        if dropout is not None:\n",
    "            self.dropout.add_module('dropout', nn.Dropout2d(self.dropout_rate))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_identity = self.identity(x)\n",
    "        x_padded = _zero_padding(x_identity, self.out_channel)\n",
    "        output = self.paths(x)\n",
    "        output = output + x_padded\n",
    "\n",
    "        output = self.dropout(self.bn_acti_layers(output))\n",
    "            \n",
    "        return output\n",
    "\n",
    "'''\n",
    "    Inception block in Inception V3\n",
    "'''\n",
    "class _InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channel, out_channels, dim_reduces, stride = 1, bn_acti = True):\n",
    "        super(_InceptionModule, self).__init__()\n",
    "        # out_channels = dict(conv1, conv3, conv5, maxpooling)\n",
    "        # dim_reduces = dict(conv3, conv5)\n",
    "        \n",
    "        assert sum(out_channels.values()) >= in_channel, 'out_channels must be greater than or equal to in_channel'\n",
    "        \n",
    "        self.in_channel = in_channel\n",
    "        self.out_channels = out_channels\n",
    "        self.dim_reduces = dim_reduces\n",
    "        self.stride = stride\n",
    "        self.bn_acti = bn_acti\n",
    "        \n",
    "        self.conv1 = _conv2d(in_channel, out_channels['conv1'], kernel_size = 1, stride = stride, bn_acti = False)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            _conv2d(in_channel, dim_reduces['conv3'], kernel_size = 1, bn_acti = True),\n",
    "            _conv2d(dim_reduces['conv3'], out_channels['conv3'], kernel_size = 3, stride = stride, padding = 1, bn_acti = False)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            _conv2d(in_channel, dim_reduces['conv5'], kernel_size = 1, bn_acti = True),\n",
    "            _conv2d(dim_reduces['conv5'], out_channels['conv5'], kernel_size = (1, 5), stride = (1, stride), padding = (0, 2), bn_acti = False),\n",
    "            _conv2d(out_channels['conv5'], out_channels['conv5'], kernel_size = (5, 1), stride = (stride, 1), padding = (2, 0), bn_acti = False)\n",
    "        )\n",
    "        \n",
    "        if stride == 1:\n",
    "            self.maxpool = nn.Sequential(\n",
    "                nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1),\n",
    "                _conv2d(in_channel, out_channels['maxpool'], kernel_size = 1, bn_acti = False)\n",
    "            )\n",
    "        elif stride == 2:\n",
    "            self.maxpool = nn.Sequential(\n",
    "                nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "                _conv2d(in_channel, out_channels['maxpool'], kernel_size = 1, bn_acti = False)\n",
    "            )\n",
    "        \n",
    "        self.bn_acti_layers = nn.Sequential()\n",
    "        if self.bn_acti:\n",
    "            self.bn_acti_layers.add_module('batchnorm', nn.BatchNorm2d(sum(out_channels.values())))\n",
    "            self.bn_acti_layers.add_module('activation', nn.ELU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        output = [self.conv1(x), self.conv3(x), self.conv5(x), self.maxpool(x)]\n",
    "        output = torch.cat(output, dim = 1)\n",
    "        \n",
    "        output = self.bn_acti_layers(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "'''\n",
    "    residual inception block in Inception V4\n",
    "'''\n",
    "class _ResInceptionModule(nn.Module):\n",
    "    def __init__(self, in_channel, out_channels, dim_reduces, bn_acti = True):\n",
    "        super(_ResInceptionModule, self).__init__()\n",
    "        # out_channels = dict(conv1, conv3, conv5, maxpooling)\n",
    "        # dim_reduces = dict(conv3, conv5)\n",
    "        \n",
    "        assert sum(out_channels.values()) >= in_channel, 'out_channels must be greater than or equal to in_channel'\n",
    "        \n",
    "        self.in_channel = in_channel\n",
    "        self.out_channels = out_channels\n",
    "        self.dim_reduces = dim_reduces\n",
    "        self.bn_acti = bn_acti\n",
    "        \n",
    "        self.conv1 = _conv2d(in_channel, out_channels['conv1'], kernel_size = 1, bn_acti = False)\n",
    "        self.conv3 = nn.Sequential(\n",
    "            _conv2d(in_channel, dim_reduces['conv3'], kernel_size = 1, bn_acti = True),\n",
    "            _conv2d(dim_reduces['conv3'], out_channels['conv3'], kernel_size = 3, padding = 1, bn_acti = False)\n",
    "        )\n",
    "        self.conv5 = nn.Sequential(\n",
    "            _conv2d(in_channel, dim_reduces['conv5'], kernel_size = 1, bn_acti = True),\n",
    "            _conv2d(dim_reduces['conv5'], out_channels['conv5'], kernel_size = (1, 5), padding = (0, 2), bn_acti = False),\n",
    "            _conv2d(out_channels['conv5'], out_channels['conv5'], kernel_size = (5, 1), padding = (2, 0), bn_acti = False)\n",
    "        )\n",
    "        self.maxpool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1),\n",
    "            _conv2d(in_channel, out_channels['maxpool'], kernel_size = 1, bn_acti = False)\n",
    "        )\n",
    "        \n",
    "        self.remap = _conv2d(sum(out_channels.values()), sum(out_channels.values()), kernel_size = 1, bn_acti = False)\n",
    "        \n",
    "        self.bn_acti_layers = nn.Sequential()\n",
    "        if self.bn_acti:\n",
    "            self.bn_acti_layers.add_module('batchnorm', nn.BatchNorm2d(sum(out_channels.values())))\n",
    "            self.bn_acti_layers.add_module('activation', nn.ELU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = [self.conv1(x), self.conv3(x), self.conv5(x), self.maxpool(x)]\n",
    "        output = torch.cat(output, dim = 1)\n",
    "        \n",
    "        x_padded = _zero_padding(x, sum(self.out_channels.values()))\n",
    "        output = x_padded + self.remap(output)\n",
    "        \n",
    "        output = self.bn_acti_layers(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b47064a5b446a5d177a709cf59fd421217b24f87"
   },
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e16f9edc234c159636521755ad95f304409799e"
   },
   "outputs": [],
   "source": [
    "class DenseUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseUNet, self).__init__()\n",
    "        self.img_head = _conv2d(1, 16, kernel_size = 3, stride = 1, padding = 1)    # 128\n",
    "        self.Encoder = nn.ModuleDict({\n",
    "            'c1': _contract_v2(in_channel = 17,\n",
    "                               out_channels = {'conv1': 4, 'conv3': 4, 'conv5': 4, 'maxpool': 8},\n",
    "                               dim_reduces = {'conv3': 4, 'conv5': 4},\n",
    "                               kernel_size = 3, \n",
    "                               stride = 1, \n",
    "                               padding = 1,\n",
    "                               num_res = 2,\n",
    "                               dropout = 0.25\n",
    "                              ),    # 128 -> 64, 20, 40\n",
    "            'c2': _contract_v2(in_channel = 60,    # 20 + 40\n",
    "                               out_channels = {'conv1': 12, 'conv3': 12, 'conv5': 12, 'maxpool': 24},\n",
    "                               dim_reduces = {'conv3': 12, 'conv5': 12},\n",
    "                               kernel_size = 3, \n",
    "                               stride = 1, \n",
    "                               padding = 1,\n",
    "                               num_res = 2,\n",
    "                               dropout = 0.5\n",
    "                              ),    # 64 -> 32, 60, 120\n",
    "            'c3': _contract_v2(in_channel = 180,    # 60 + 120\n",
    "                               out_channels = {'conv1': 36, 'conv3': 36, 'conv5': 36, 'maxpool': 72},\n",
    "                               dim_reduces = {'conv3': 36, 'conv5': 36},\n",
    "                               kernel_size = 3, \n",
    "                               stride = 1, \n",
    "                               padding = 1,\n",
    "                               num_res = 2,\n",
    "                               dropout = 0.5\n",
    "                              ),    # 32 -> 16, 180, 360\n",
    "            'c4': _contract_v2(in_channel = 540,    # 180 + 360\n",
    "                               out_channels = {'conv1': 108, 'conv3': 108, 'conv5': 108, 'maxpool': 216},\n",
    "                               dim_reduces = {'conv3': 108, 'conv5': 108},\n",
    "                               kernel_size = 3, \n",
    "                               stride = 1, \n",
    "                               padding = 1,\n",
    "                               num_res = 3,\n",
    "                               dropout = 0.5\n",
    "                              ),    # 16 -> 8, 540, 1080\n",
    "        })\n",
    "        self.Decoder = nn.ModuleDict({\n",
    "            'e1': _expand_v2(1080, 360, 360, kernel_size = 3, stride = 1, padding = 1, num_res = 3, dropout = 0.5),    # 8 -> 16, 360\n",
    "            'e2': _expand_v2(360, 120, 120, kernel_size = 3, stride = 1, padding = 1, num_res = 2, dropout = 0.5),    # 16 -> 32, 120\n",
    "            'e3': _expand_v2(120, 40, 40, kernel_size = 3, stride = 1, padding = 1, num_res = 2, dropout = 0.5),    # 32 -> 64, 40\n",
    "            'e4': _expand_v2(40, 17, 17, kernel_size = 3, stride = 1, padding = 1, num_res = 2, dropout = 0.5),    # 64 -> 128, 17\n",
    "        })\n",
    "        self.tail = nn.Sequential(\n",
    "            _conv2d(17, 16, kernel_size = 3, stride = 1, padding = 1),\n",
    "            _conv2d(16, 1, kernel_size = 1, bn_acti = False)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        dep = x[1].expand_as(x[0])\n",
    "        head = torch.cat([dep, self.img_head(x[0])], dim = 1)\n",
    "        \n",
    "        c1 = self.Encoder['c1'](head)\n",
    "        c2 = self.Encoder['c2'](torch.cat(c1, dim = 1))\n",
    "        c3 = self.Encoder['c3'](torch.cat(c2, dim = 1))\n",
    "        c4 = self.Encoder['c4'](torch.cat(c3, dim = 1))\n",
    "        \n",
    "        e1 = self.Decoder['e1'](c4[1], c3[1])\n",
    "        e2 = self.Decoder['e2'](e1, c2[1])\n",
    "        e3 = self.Decoder['e3'](e2, c1[1])\n",
    "        e4 = self.Decoder['e4'](e3, head)\n",
    "        \n",
    "        tail = self.tail(e4)\n",
    "        \n",
    "        return tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0db4ccf1b7eaa7259ed12d4581f7942b44deb045"
   },
   "outputs": [],
   "source": [
    "class dsDSEUNeXt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dsDSEUNeXt, self).__init__()\n",
    "        self.img_head = _conv2d(1, 16, kernel_size = 3, stride = 1, padding = 1)    # 128\n",
    "        self.Encoder = nn.ModuleDict({\n",
    "            'c1': _contract_v2(in_channel = 17,\n",
    "                               incept_channels = {'conv1': 4, 'conv3': 4, 'conv5': 4, 'maxpool': 8},\n",
    "                               dim_reduces = {'conv3': 4, 'conv5': 4},\n",
    "                               inter_channel = 20, \n",
    "                               out_channel = 40,\n",
    "                               kernel_size = 3, \n",
    "                               stride = 1, \n",
    "                               padding = 1,\n",
    "                               groups = 20,\n",
    "                               num_res = 2,\n",
    "                               dropout = None,\n",
    "                               scSE = True),    # 128 -> 64, 20, 40\n",
    "            'c2': _contract_v2(in_channel = 60,    # 20 + 40\n",
    "                               incept_channels = {'conv1': 12, 'conv3': 12, 'conv5': 12, 'maxpool': 24},\n",
    "                               dim_reduces = {'conv3': 12, 'conv5': 12},\n",
    "                               inter_channel = 60,\n",
    "                               out_channel = 120,\n",
    "                               kernel_size = 3, \n",
    "                               stride = 1, \n",
    "                               padding = 1,\n",
    "                               groups = 60,\n",
    "                               num_res = 3,\n",
    "                               dropout = None,\n",
    "                               scSE = True),    # 64 -> 32, 60, 120\n",
    "            'c3': _contract_v2(in_channel = 180,    # 60 + 120\n",
    "                               incept_channels = {'conv1': 36, 'conv3': 36, 'conv5': 36, 'maxpool': 72},\n",
    "                               dim_reduces = {'conv3': 36, 'conv5': 36},\n",
    "                               inter_channel = 180,\n",
    "                               out_channel = 360,\n",
    "                               kernel_size = 3, \n",
    "                               stride = 1, \n",
    "                               padding = 1,\n",
    "                               groups = 180,\n",
    "                               num_res = 3,\n",
    "                               dropout = None,\n",
    "                               scSE = True),    # 32 -> 16, 180, 360\n",
    "            'c4': _contract_v2(in_channel = 540,    # 180 + 360\n",
    "                               incept_channels = {'conv1': 108, 'conv3': 108, 'conv5': 108, 'maxpool': 216},\n",
    "                               dim_reduces = {'conv3': 108, 'conv5': 108},\n",
    "                               inter_channel = 540,\n",
    "                               out_channel = 1080,\n",
    "                               kernel_size = 3, \n",
    "                               stride = 1, \n",
    "                               padding = 1,\n",
    "                               groups = 540,\n",
    "                               num_res = 4,\n",
    "                               dropout = None,\n",
    "                               scSE = True)    # 16 -> 8, 540, 1080\n",
    "        })\n",
    "        self.Decoder = nn.ModuleDict({\n",
    "            'e1': _expand_v2(1080, 360, 180, 360, kernel_size = 3, stride = 1, padding = 1, groups = 180, \n",
    "                             num_res = 4, dropout = None, scSE = True),    # 8 -> 16, 360\n",
    "            'e2': _expand_v2(360, 120, 60, 120, kernel_size = 3, stride = 1, padding = 1, groups = 60, \n",
    "                             num_res = 3, dropout = None, scSE = True),    # 16 -> 32, 120\n",
    "            'e3': _expand_v2(120, 40, 20, 40, kernel_size = 3, stride = 1, padding = 1, groups = 20, \n",
    "                             num_res = 3, dropout = None, scSE = True),    # 32 -> 64, 40\n",
    "            'e4': _expand_v2(40, 17, 8, 17, kernel_size = 3, stride = 1, padding = 1, groups = 8, \n",
    "                             num_res = 2, dropout = None, scSE = True)   # 64 -> 128, 17\n",
    "        })\n",
    "        self.tail = nn.Sequential(\n",
    "            _conv2d(17, 16, kernel_size = 3, stride = 1, padding = 1),\n",
    "            _scSE(16),\n",
    "            _conv2d(16, 1, kernel_size = 1, bn_acti = False)\n",
    "        )\n",
    "        self.binary_classifier = nn.Sequential(\n",
    "            _ResBlock(1080, 270, 540, kernel_size = 3, stride = 2, padding = 1),\n",
    "            _ResBlock(540, 135, 270, kernel_size = 3, stride = 2, padding = 1),\n",
    "            _scSE(270),\n",
    "            nn.AvgPool2d(kernel_size = 2),\n",
    "            _conv2d(270, 1, kernel_size = 1)\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        dep = x[1].expand_as(x[0])\n",
    "        head = torch.cat([dep, self.img_head(x[0])], dim = 1)\n",
    "        \n",
    "        c1 = self.Encoder['c1'](head)\n",
    "        c2 = self.Encoder['c2'](torch.cat(c1, dim = 1))\n",
    "        c3 = self.Encoder['c3'](torch.cat(c2, dim = 1))\n",
    "        c4 = self.Encoder['c4'](torch.cat(c3, dim = 1))\n",
    "        \n",
    "        e1 = self.Decoder['e1'](c4[1], c3[1])\n",
    "        e2 = self.Decoder['e2'](e1, c2[1])\n",
    "        e3 = self.Decoder['e3'](e2, c1[1])\n",
    "        e4 = self.Decoder['e4'](e3, head)\n",
    "        \n",
    "        tail = self.tail(e4)\n",
    "        \n",
    "        binary_pred = self.binary_classifier(c4[1])\n",
    "        \n",
    "        return tail, binary_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28d01e76cc90db74e2968cad2aa5e806e552c215"
   },
   "source": [
    "## Customize Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce0f6f5fb6ee3f0e9b3471801a224388b44a9db0"
   },
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/solved-class-weight-for-bceloss/3114\n",
    "def weighted_BCELoss(output, target, weights=None):    \n",
    "    if weights is not None:\n",
    "        assert len(weights) == 2\n",
    "        \n",
    "        loss = weights[1] * (target * torch.log(output)) + \\\n",
    "               weights[0] * ((1 - target) * torch.log(1 - output))\n",
    "    else:\n",
    "        loss = target * torch.log(output) + (1 - target) * torch.log(1 - output)\n",
    "\n",
    "    return torch.neg(torch.mean(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da9ab580644ffc4a7195f38ffb7810e725c1c7d0"
   },
   "outputs": [],
   "source": [
    "# https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n",
    "# a safe Binary Focal Loss function\n",
    "class BFWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, gamma = 2, alpha = None, size_average = True):\n",
    "        super(BFWithLogitsLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        # alpha[i] is the weight of class i\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha)\n",
    "        else:\n",
    "            self.alpha = torch.ones(2)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        if not (target.shape == logits.shape):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\".\\\n",
    "                             format(target.size(), logits.size()))\n",
    "        \n",
    "        max_val = (-logits).clamp(min = 0)\n",
    "        BCE = logits - logits * target + max_val + ((-max_val).exp() + (-max_val - logits).exp()).log()\n",
    "\n",
    "        if logits.is_cuda and not self.alpha.is_cuda:\n",
    "            self.alpha = self.alpha.cuda()\n",
    "        \n",
    "        alpha_indexed = torch.index_select(self.alpha, dim = 0, index = target.long().view(-1)).reshape_as(target)\n",
    "        pt = torch.sigmoid(-logits * (target * 2 - 1))\n",
    "        \n",
    "        loss = pt ** self.gamma * alpha_indexed * BCE\n",
    "        \n",
    "#         has_nan(loss, 'BFWithLogitsLoss function')\n",
    "        \n",
    "        if self.size_average: \n",
    "            return loss.mean()\n",
    "        return loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66d7f3b34c69dee24c083565e9a6b25be995efec"
   },
   "outputs": [],
   "source": [
    "# https://github.com/bermanmaxim/LovaszSoftmax/blob/master/pytorch/lovasz_losses.py\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "        Binary Lovasz hinge loss\n",
    "          logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "          labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "          ignore: label to ignore\n",
    "          \n",
    "        replaced F.relu to F.elu, in hoping for pushing the pred to a large margin even \n",
    "        when the original loss arrives at 0.\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels - 1.\n",
    "    errors = (1. - logits * signs)\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.elu(errors_sorted) + 1, grad)\n",
    "    return loss\n",
    "\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(np.isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n\n",
    "\n",
    "class LovaszHingeLoss(nn.Module):\n",
    "    def __init__(self, per_image = True, ignore = None):\n",
    "        super(LovaszHingeLoss, self).__init__()\n",
    "        self.per_image = per_image\n",
    "        self.ignore = ignore\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Binary Lovasz hinge loss\n",
    "          logits: [B, 1, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "          labels: [B, 1, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "          per_image: compute the loss per image instead of per batch\n",
    "          ignore: void class id\n",
    "        \"\"\"\n",
    "        logits = logits.squeeze(1)\n",
    "        targets = targets.squeeze(1)\n",
    "        if self.per_image:\n",
    "            loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), self.ignore))\n",
    "                              for log, lab in zip(logits, targets))\n",
    "        else:\n",
    "            loss = lovasz_hinge_flat(*flatten_binary_scores(logits, targets, self.ignore))\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "126a95ca21c10c147bded6e29df0ea1dea4e43ed"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a17170be1715d30c024f24c840f66b25f925a768",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -la ../input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc47933834bab9a3e5431bb288422056261af2a3"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'epochs': 80,\n",
    "    'learning_rate': 0.1,\n",
    "    'use_cuda': torch.cuda.is_available(),\n",
    "    'batch_size': 16,\n",
    "    'k_fold': 5\n",
    "}\n",
    "\n",
    "model = dsDSEUNeXt()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = args['learning_rate'], momentum = 0.9, weight_decay = 1e-5)\n",
    "\n",
    "fold = 0\n",
    "last_cycle = -1\n",
    "# model_dir = '../input/tgs-salt-p1-final-fold{}-cycle{}/model'.format(fold, last_cycle)\n",
    "# filename = model.__class__.__name__ + '_fold{}'.format(fold) + '_cycle{}'.format(last_cycle)\n",
    "# ckpt = load_checkpoint(is_best = False, dir_path = model_dir, filename = filename)\n",
    "# print('epoch: ', ckpt['epoch'])\n",
    "# print('best validation metrics: \\n', ckpt['best_valid_metrics'])\n",
    "\n",
    "# model.load_state_dict(ckpt['model'])\n",
    "# optimizer.load_state_dict(ckpt['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e3d5edfa5474f3e83f7ebb205e96e2b58f894134",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('model summary:')\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('total params:', total_params)\n",
    "print(model)\n",
    "\n",
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# loss = BFWithLogitsLoss(gamma = 0.8, alpha = (0.4, 0.6))\n",
    "losses = {'non_empty': LovaszHingeLoss(), \n",
    "          'binary': nn.BCEWithLogitsLoss(), \n",
    "          'all_segs': LovaszHingeLoss()}\n",
    "\n",
    "if args['use_cuda']:\n",
    "    model = model.cuda()\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "#                                                           mode = 'max', \n",
    "#                                                           factor = 0.5,\n",
    "#                                                           patience = 8, \n",
    "#                                                           verbose = True,\n",
    "#                                                           threshold = 10e-4,\n",
    "#                                                           cooldown = 2,\n",
    "#                                                           min_lr = 1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                          T_max = 80,\n",
    "                                                          eta_min = 1e-5)\n",
    "\n",
    "train_dataloaders = get_dataloader(train_dataset, args, train_df.coverage_class.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1338b09c2343352c4dca99b4588a9e6c4c2e9199",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "metrics = training(model, losses, optimizer, train_dataloaders, args, lr_scheduler, \n",
    "                   fold = fold, start_cycle = last_cycle + 1, model_dir = './model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "344ebe5bd2e3956a37659a4506c1d216a398d29f"
   },
   "outputs": [],
   "source": [
    "def plot_metrics(metrics: dict):\n",
    "    colors = ['r', 'b', 'g', 'c', 'm', 'k', 'aquamarine', 'orange']\n",
    "       \n",
    "    plt.figure(figsize=(18,10))\n",
    "    max_value = 0.0\n",
    "    for i, item in enumerate(metrics.items()):\n",
    "        color = colors[i]\n",
    "        key, values = item\n",
    "        x = np.arange(len(values))\n",
    "        plt.plot(x, values, color, label = key)\n",
    "        max_value = max(max_value, max(values))\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.yticks(np.arange(0.0, max_value, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b5ba7453b571859e09de46daf82141b44554f20"
   },
   "outputs": [],
   "source": [
    "plot_metrics(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
